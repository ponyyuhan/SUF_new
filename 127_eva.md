端到端 breakdown：完整且详细的实验方案（用于回应“归因不清 / Figure1 vs Table1 不一致”）

下面给的是“你跑出来就能直接写进论文”的方案。目标是让 reviewer 看到：

* **（归因）** 端到端加速到底来自哪里：nonlinear kernels 占比从 X% → Y%？
* **（对齐）** Figure 1 的 per‑gate speedup（~1.6–2.5×）为什么只转化成 Table 1 的 1.23–1.42×？（用 Amdahl 定量解释）
* **（padding）** mask-independent padding 的最坏 3.8× per-gate 开销，端到端到底影响多大？什么时候会伤？

### 3.1 要回答的问题（你最终在 paper 里要展示什么）

对每个模型/设置（Sigma vs FuseFSS），输出：

1. **Online latency breakdown（ms）**

    * Linear layers（MatMul/Conv/attention projections）
    * Attention softmax path（exp/normalize，含你的 compiled nexp/rsqrt 等）
    * LayerNorm path
    * Activation（GELU/SiLU）
    * Trunc/Rescale / wrap correction / helper ops
    * Misc（masking、reshape、dispatch、框架开销）
    * **Comm wait（如果有显著同步等待）**

2. **Online communication breakdown（bytes/GB）**（同样按上面的类别分摊）

3. **Rounds / message count breakdown（可选但很加分）**
   reviewer 多次提 rounds/latency，能给会很强。

4. **Amdahl reconciliation（核心一行就够）**

    * 令 `f = nonlinear 部分在 Sigma 的 wall-time 占比`
    * 令 `s = nonlinear 部分的加速比（从 microbench 得到）`
    * 预测整体加速：`Speedup_pred = 1 / ((1-f) + f/s)`
    * 把 `Speedup_pred` 与实测 `Speedup_meas` 并列（差异解释为 overlap、调度、cache 等）

你在主文里只需要放一句非常“硬”的 reconciliation：

> “Nonlinear kernels account for X% of Sigma’s online runtime, reduced to Y% in FuseFSS; hence a per-gate 1.6–2.5× speedup translates to an end-to-end 1.23–1.42×.”
> 详细 breakdown 图/表放 Appendix。

### 3.2 实验矩阵（建议跑哪些点）

**最低成本、最大收益（直接对应 reviewer）**

* 模型：BERT-base、GPT-2（你 1/26 已经有 seq/batch 扩展）
* Seq：128 / 256 / 512
* Batch：

    * BERT-base：1 / 2 / 4 / 8（你目前还缺，强烈建议补这组）
    * GPT-2：1 / 2 / 4 / 8（你已有）

**可选加强（如果你有算力/时间）**

* 再加一个“线性主导”的模型（如 GPT‑Neo 或更大的模型）在 seq=512 下的 breakdown，验证“线性主导时收益怎么变化”。
* 尝试 seq=1024（如果 Sigma OOM，就明确写：baseline OOM，FuseFSS 是否能跑也写清楚；这类负结果对 reviewer 也很有说服力）。

### 3.3 计时与归因：怎么做才“可写进 ICML”

#### Step A：在运行时加 profiling scope（推荐做法）

在你的 C++/CUDA runtime（或 Python 绑定层）里，对每个“高层算子”包一层 scope：

* `SCOPE("Linear")`：每个 MatMul/GEMM/Conv/Attention matmul
* `SCOPE("Activation::GELU")` / `SCOPE("Activation::SiLU")`
* `SCOPE("LayerNorm")`
* `SCOPE("Softmax")`（内部再细分 `exp/rsqrt` 等也行）
* `SCOPE("TruncRescale")`
* `SCOPE("FSS::PackedCompare")`
* `SCOPE("FSS::IntervalLookup")`
* `SCOPE("Other")`

**计时用两套（避免 reviewer 挑刺）：**

1. **Wall-clock（主报告）**：用 `std::chrono` 包住整个 scope（含可能的通信等待/同步）。
2. **GPU kernel time（辅助报告）**：用 `cudaEventRecord` 在 scope 内部只记 kernel 时间（不含网络/CPU 等）。

最终在论文里主要用 wall-clock breakdown（因为 end-to-end speedup 也是 wall-clock），GPU time 作为解释补充即可。

#### Step B：通信 bytes/rounds 的归因（关键难点，但做了就非常强）

做法：在通信层（send/recv）维护一个 thread-local “当前 scope tag”。

* `SCOPE(name)` 进入时设置 `tls_current_tag=name`，退出恢复。
* `send(bytes)` 时：`comm_bytes[tls_current_tag] += bytes`
* `recv(bytes)` 同样累计
* 如果你的协议有“round”的概念：可以按 barrier / flush / sync 次数计 rounds，或直接 message count。

这样你能输出 per-scope 的 bytes 与 message/round 计数。

#### Step C：两方如何合并（Sigma/SUF 是两方同时跑）

你要避免 reviewer 说“你只看了一方”。建议：

* 每次实验保存两方日志 `p0.json`, `p1.json`，每个 scope 有 time/bytes。
* 合并规则：

    * **Latency（ms）**：取 `max(p0_time, p1_time)`（端到端由慢方决定）
    * **Bytes**：取 `p0_send+p0_recv+p1_send+p1_recv` 或者你统一定义“总传输量”即可（关键是定义一致）。
* 输出一个 `breakdown_merged.json`，供画图与出表。

### 3.4 统计与可复现（避免 reviewer 质疑“噪声”）

对每个 setting（model, seq, batch, system）：

* 预热 1 次（丢弃）
* 正式跑 **5 次**（或 7 次），报告：

    * median（主表）
    * [p25, p75]（appendix，可选）
* 固定：

    * GPU 绑定（你现在已经做了）
    * OMP 线程数
    * SIGMA/SUF 的 keybuf/mempool 配置（保持一致）
* 日志写入：建议 JSON（机器可读）+ 关键摘要打印到 stdout（human readable）

### 3.5 输出形态：你在论文里怎么放才不超 8 页

**主文（1 句 + 1 个小表/小图，二选一）**

* 只放一句 reconciliation（X%→Y%）+ 指向 Appendix breakdown 图。
* 或放一个非常小的 2×2 表：

    * 行：Sigma / FuseFSS
    * 列：Nonlinear share / Linear share（百分比）
      这张表占地极小，但能一锤定音回应 reviewer。

**Appendix（完整图/表）**

* 1 张 stacked bar：每个 setting 一根柱（Sigma vs FuseFSS），颜色按 scope。
* 1 张 stacked bar：bytes breakdown（同理）。
* 1 张表：Amdahl predicted vs measured（可选，但非常有说服力）。

### 3.6 专门回答 reviewer 的 padding red-flag（你应该怎么测）

你已经有 per-gate 的 padding ablation（Table 4）显示 3.8× 的 worst-case gate-time。reviewer 关心的是：**这会不会抵消 end-to-end 加速？什么时候会严重？**

建议补一个 **端到端 padding ablation**（非常便宜）：

* 选 BERT-base、GPT-2 各 1 个 setting（例如 seq=128 与 seq=512）
* 开两个编译/运行模式：

    1. **Shape-independent padding ON**（默认安全模式）
    2. **Padding OFF / mask-dependent shape（仅作为不安全 ablation）**
* 报告：end-to-end online time、online comm、以及 breakdown 中 “Nonlinear scope” 的变化
* 文字结论写法（建议模板）：

    * “padding 会显著影响某些小 gate 的 per-gate 时间，但端到端影响取决于该 gate 的调用次数与其在总 runtime 中的占比；我们在 XXX setting 下观测到端到端仅增加 Z% / 或显著增加 Z%。”

这能把 reviewer 的“red flag”从“你没解释”变成“你解释并量化了”。

---

## 我建议你额外补的 1–2 个实验（如果你想一次性把 reviewer 风险压到最低）

不是必须，但很划算：

1. **BERT-base 的 batch sweep（1/2/4/8）**
   你现在只有 GPT‑2 的 batch sweep。reviewer 明确点名 batch size 固定 1。补 BERT-base batch sweep 能把这个点彻底封死。

2. **（可选）BERT-base 或 GPT‑2 的 seq=1024（如果 baseline OOM 也可以）**
   如果 Sigma OOM，你就把 “baseline 受限于 memory / keybuf” 明确写出来；这在顶会审稿里通常不是扣分项，反而显得你诚实且理解系统瓶颈。
